"""
LLM Service for Elefante
Handles interactions with Language Models (OpenAI, etc.) for memory consolidation and curation.
"""

import json
import os
from typing import List, Dict, Any, Optional
from openai import AsyncOpenAI
from src.utils.config import get_config
from src.utils.logger import get_logger

logger = get_logger(__name__)

class LLMService:
    """
    Service for interacting with LLMs.
    Currently supports OpenAI compatible APIs.
    """
    
    def __init__(self):
        self.config = get_config()
        self.client = None
        self.model = self.config.elefante.llm.model
        self._initialize_client()
        
    def _initialize_client(self):
        """Initialize OpenAI client"""
        api_key = self.config.elefante.llm.api_key or os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.warning("No LLM API key found. LLM features will be disabled.")
            return
            
        self.client = AsyncOpenAI(api_key=api_key)
        logger.info(f"LLM Service initialized with model: {self.model}")

    async def generate_response(self, system_prompt: str, user_prompt: str) -> str:
        """Generate a text response from the LLM"""
        if not self.client:
            return "LLM Service not configured."

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=self.config.elefante.llm.temperature,
                max_tokens=self.config.elefante.llm.max_tokens
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"LLM generation failed: {e}")
            return f"Error generating response: {e}"

    async def extract_entities(self, content: str) -> List[Dict[str, str]]:
        """
        Extract entities and tags from content using LLM.
        Returns a list of dicts: {"name": "X", "type": "Y"}
        """
        if not self.client:
            return []

        system_prompt = """
        You are an expert knowledge graph engineer. Extract key entities from the user's text.
        Return ONLY a JSON array of objects with 'name' and 'type' fields.
        Types should be one of: person, project, technology, organization, location, event, concept, file, task.
        Example: [{"name": "Elefante", "type": "project"}, {"name": "Python", "type": "technology"}]
        """

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": content}
                ],
                response_format={"type": "json_object"},
                temperature=0.0
            )
            
            result = json.loads(response.choices[0].message.content)
            # Handle both {"entities": [...]} and [...] formats
            if "entities" in result:
                return result["entities"]
            elif isinstance(result, list):
                return result
            else:
                # Try to find a list in the values
                for val in result.values():
                    if isinstance(val, list):
                        return val
                return []
                
        except Exception as e:
            logger.error(f"Entity extraction failed: {e}")
            return []

    async def analyze_memory(self, content: str) -> Dict[str, Any]:
        """
        Analyze memory content to extract deep cognitive structure.
        Returns a dictionary matching the CognitiveAnalysis schema.
        """
        if not self.client:
            # Fallback for no API key
            return {
                "title": content[:50] + "..." if len(content) > 50 else content,
                "summary": content,
                "intent": "statement",
                "emotional_context": {"valence": 0.0, "arousal": 0.0, "mood": "Neutral"},
                "entities": [],
                "relationships": [],
                "strategic_insight": None,
                "action": "ADD"
            }

        system_prompt = """
        You are the Cortex of an advanced AI memory system. 
        Analyze the incoming memory to extract its "Cognitive Delta"â€”the structural change it imposes on the World Model.
        
        Output JSON ONLY matching this schema:
        {
            "title": "Concise, semantic title (e.g., 'User Language Preference')",
            "summary": "Brief summary of the content",
            "intent": "teaching" | "venting" | "planning" | "reflecting" | "deciding" | "querying" | "statement",
            "emotional_context": {
                "valence": float (-1.0 to 1.0),
                "arousal": float (0.0 to 1.0),
                "mood": "string (e.g., Frustrated, Determined)"
            },
            "entities": [
                {"name": "EntityName", "type": "Type", "description": "Context"}
            ],
            "relationships": [
                {"source": "EntityName", "target": "EntityName", "type": "RELATIONSHIP_TYPE", "reason": "Context"}
            ],
            "strategic_insight": "Actionable takeaway or rule (optional)",
            "action": "ADD" | "UPDATE" | "IGNORE"
        }
        
        RELATIONSHIP GUIDELINES:
        - Use semantic types: LOVES, HATES, BLOCKS, ENABLES, DEPENDS_ON, IS_A, HAS_PART.
        - Connect the 'User' entity to concepts they express opinions about.
        
        Example Input: "I hate Python's GIL, it slows everything down. We should use Rust."
        Example Output:
        {
            "title": "Python GIL vs Rust",
            "summary": "User expresses frustration with Python's GIL performance and suggests Rust as an alternative.",
            "intent": "deciding",
            "emotional_context": {"valence": -0.6, "arousal": 0.7, "mood": "Frustrated"},
            "entities": [
                {"name": "User", "type": "Person"},
                {"name": "Python", "type": "Technology"},
                {"name": "Rust", "type": "Technology"},
                {"name": "GIL", "type": "Concept"}
            ],
            "relationships": [
                {"source": "User", "target": "Python", "type": "DISLIKES", "reason": "GIL performance"},
                {"source": "User", "target": "Rust", "type": "PREFERS", "reason": "Performance"},
                {"source": "Python", "target": "GIL", "type": "HAS_PART"}
            ],
            "strategic_insight": "User prefers Rust over Python for performance-critical tasks due to GIL limitations.",
            "action": "ADD"
        }
        """

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": content}
                ],
                response_format={"type": "json_object"},
                temperature=0.0
            )
            
            result = json.loads(response.choices[0].message.content)
            return result
            
        except Exception as e:
            logger.error(f"Memory analysis failed: {e}")
            # Fallback
            return {
                "title": content[:50] + "...",
                "summary": content,
                "intent": "statement",
                "emotional_context": {"valence": 0.0, "arousal": 0.0, "mood": "Error"},
                "entities": [],
                "relationships": [],
                "strategic_insight": None,
                "action": "ADD"
            }

# Global singleton
_llm_service: Optional[LLMService] = None

def get_llm_service() -> LLMService:
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMService()
    return _llm_service
